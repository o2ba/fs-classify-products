{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Establishing a Baseline\n",
    "\n",
    "### Objective\n",
    "\n",
    "The purpose of this section is to provide with a baseline model which we will use to benchmark subsequent models. A baseline provides a reference point to evaluate whether future enhancements (like feature engineering, hyperparameter tuning, or algorithm changes) actually deliver meaningful improvements.\n",
    "\n",
    "For this task, we will use a Random Forest Classifier due to its robustness, ability to handle high-dimensional data, and minimal preprocessing requirements.\n",
    "\n",
    "### Data Extraction\n",
    "We will first extract all the data required. We have a helper function for this which will drop the id as well as this is not needed."
   ],
   "id": "d018ba887e2b206"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from src.loader.data_loader import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "data: pd.DataFrame = load_dataset(filepath=\"data/ClassifyProducts.csv\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Preparation\n",
    "\n",
    "We begin by splitting our dataset into two key components:\n",
    "\n",
    "- `data_features`: Contains all feature columns used as inputs for the model.\n",
    "- `data_targets`: Contains the target variable we aim to predict — in this case, the product category."
   ],
   "id": "f9f3e4b05ef312ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_features: pd.DataFrame = data.drop(columns=[\"target\"])\n",
    "data_targets: pd.Series = data[\"target\"]"
   ],
   "id": "639476dacd5dfbe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train-Test Split\n",
    "\n",
    "To evaluate our model's ability to generalize to unseen data, we divide the dataset into a training set and a testing set. We allocate 80% of the data for training and 20% for testing.\n",
    "\n",
    "We also apply stratification to ensure that the distribution of classes in the target variable remains consistent across both sets. This is particularly important when dealing with potential class imbalances."
   ],
   "id": "d043c7527508ea11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_targets, test_targets = train_test_split(\n",
    "    data_features, data_targets, test_size=0.2, random_state=42, stratify=data_targets\n",
    ")"
   ],
   "id": "2b3dbddf2213816c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training the baseline model\n",
    "\n",
    "We initialize a `RandomForestClassifier` with default parameters, setting only the number of trees (`n_estimators=100`) and a `random_state` for reproducibility.\n",
    "\n",
    "The first challenge is to determine the n_estimators (number of trees). For this, we will draw a performance curve to determine the point of diminishing returns"
   ],
   "id": "c028a423298ef278"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List\n",
    "\n",
    "# Define the range of tree counts to evaluate\n",
    "number_of_trees_list: List[int] = [10, 50, 100, 200, 300, 400, 500, 700, 1000]\n",
    "\n",
    "# Initialize a list to store accuracy scores (We will draw the curve from these)\n",
    "test_set_accuracies: List[float] = []\n",
    "\n",
    "for number_of_trees in number_of_trees_list:\n",
    "    random_forest_model: RandomForestClassifier = RandomForestClassifier(\n",
    "        n_estimators=number_of_trees,\n",
    "        random_state=65\n",
    "    )\n",
    "\n",
    "    random_forest_model.fit(train_features, train_targets)\n",
    "\n",
    "    test_set_predictions = random_forest_model.predict(test_features)\n",
    "\n",
    "    test_set_accuracy: float = accuracy_score(test_targets, test_set_predictions)\n",
    "    test_set_accuracies.append(test_set_accuracy)\n",
    "\n",
    "    print(f\"Random Forest with {number_of_trees} trees --> Test Set Accuracy: {test_set_accuracy:.4f}\")\n",
    "\n",
    "# Plot the curve of test set accuarcy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(number_of_trees_list, test_set_accuracies, marker='o')\n",
    "plt.title(\"Test Set Accuracy vs Number of Trees in Random Forest\")\n",
    "plt.xlabel(\"Number of Trees (n_estimators)\")\n",
    "plt.ylabel(\"Test Set Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "a53f566b08d2114b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Interpretation of results:\n",
    "- A significant improvement in accuracy is observed when increasing the number of trees from 10 to 50, where accuracy rises from 0.7830 to 0.8086.\n",
    "- Further increases up to 200-300 trees yield marginal gains, peaking at an accuracy of 0.8129.\n",
    "- Beyond 300 trees, additional trees do not contribute to meaningful improvements in predictive performance. Minor fluctuations in accuracy (within ±0.0015) are attributable to randomness and model variance, despite the stability offered by ensemble methods.\n",
    "- The curve clearly demonstrates a performance plateau after approximately 300 trees, indicating the point of diminishing returns.\n",
    "\n",
    "For the baseline model, we will use 300 trees."
   ],
   "id": "73c39b7db920965f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize and train baseline Random Forest\n",
    "baseline_model = RandomForestClassifier(n_estimators=300, random_state=65)\n",
    "baseline_model.fit(train_features, train_targets)\n",
    "\n",
    "# Predict on test set\n",
    "test_predictions = baseline_model.predict(test_features)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(test_targets, test_predictions))"
   ],
   "id": "ca0397a6579677c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "     Class_1       0.79      0.43      0.56       386\n",
    "     Class_2       0.72      0.88      0.80      3224\n",
    "     Class_3       0.64      0.51      0.57      1601\n",
    "     Class_4       0.87      0.43      0.58       538\n",
    "     Class_5       0.97      0.97      0.97       548\n",
    "     Class_6       0.93      0.94      0.94      2827\n",
    "     Class_7       0.77      0.59      0.67       568\n",
    "     Class_8       0.88      0.94      0.91      1693\n",
    "     Class_9       0.85      0.89      0.87       991\n",
    "\n",
    "    accuracy                           0.81     12376\n",
    "   macro avg       0.83      0.73      0.76     12376\n",
    "weighted avg       0.81      0.81      0.80     12376"
   ],
   "id": "e87aa1d42d7d94b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
